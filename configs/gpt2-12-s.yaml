---
# small variant config for transformer GPT2-12 (12 decoder layers)
# Date: 2024-01-05
#   GPT2 specific
#   --- 15 min ---
#   256 / 2048 / 6 / 745 ~= 8.9GB
#   256 / 2048 / 6 / 1024 ~= 12.23GB
#   512 / 2048 / 6 / 1024 (8 head) ~= 14.9GB
#   512 / 2048 / 8 / 1024 (8 head) ~= 18.8GB
#   512 / 2048 / 12 / 512 (8 head) ~= 19.1GB
#   512 / 2048 / 24 / 256 (8 head) ~= 18.9GB
#   --- 10s --- 
#   128 / 512 / 12 / 16 ~= 12.92GB
#
#   lr_base = 11.3137
#   lr ~= lr_base * sqrt(dim_base) 

# dataset setting
load_data: True # False -> everytime we shuffle dataset
normalize_data: True
vectorize_data: True
style_vectorize: 'patchify'
vectorize_window_size: 8

# model setting
model_class: 'gpt2'
dim_base: 128
dropout: 0.1
num_attn_head: 8
#    Transformer specific
num_encoder_layer: 0
num_decoder_layer: 12
dim_feedforward: 2048 # DiT default ratio: 4 * dim_base

# diffusion setting
dpm_solver_sample: True
num_diffusion_step: 1000
num_sampling_step: 50 # After training, can use 100~200, or 1000. 
diffusion_objective: 'pred_v'
learn_variance: False # No need for dpm-solver
sigma_small: True
beta_schedule_type: 'cosine'
ddim_sampling_eta: 0.

# loss
mse_loss: True
rescale_learned_variance: True
only_central: False

# train setting
num_train_step: 20000
save_and_sample_every: 2000
val_every: 500
num_sample: 400

train_batch_size: 256 # a more or less approp. batch size for 23GB GPU
train_lr: 0.0001
adam_betas: [0.9, 0.999]

gradient_accumulate_every: 1
ema_update_every: 5
ema_decay: 0.9999
amp: True # automatic mixed precision
mixed_precision_type: 'fp16'
split_batches: True