---
# standard config for transformer GPT2-24 (24 decoder layers)
# larger model, using larger patch size to reduce complexity
# Date: 2024-01-21

# dataset setting
load_data: True # False -> everytime we shuffle dataset
normalize_data: True
vectorize_data: True
style_vectorize: 'patchify'
vectorize_window_size: 15

# model setting
model_class: 'gpt2'
dim_base: 256
dropout: 0.1
num_attn_head: 8
#    Transformer specific
num_encoder_layer: 0
num_decoder_layer: 24
dim_feedforward: 2048 # DiT default ratio: 4 * dim_base

# diffusion setting
num_diffusion_step: 4000
num_sampling_step: 400
diffusion_objective: 'pred_v'
learn_variance: True
sigma_small: True
beta_schedule_type: 'cosine'
ddim_sampling_eta: 0.

# loss
mse_loss: True
rescale_learned_variance: True
only_central: False

# train setting
num_train_step: 50000
save_and_sample_every: 5000
val_every: 250
num_sample: 256

train_batch_size: 256 # a more or less approp. batch size for 23GB GPU
train_lr: 0.0001
adam_betas: [0.9, 0.999]

gradient_accumulate_every: 1
ema_update_every: 5
ema_decay: 0.9999
amp: True # automatic mixed precision
mixed_precision_type: 'fp16'
split_batches: True